{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import json\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from vocabulary import Vocabulary\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "START_TOKEN = \"^\"\n",
    "END_TOKEN = \"_\"\n",
    "IGNORE_INDEX_VALUE = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Definitions \n",
    "\n",
    "Data Model:\n",
    "- Raw data\n",
    "- Vectorizer\n",
    "- Vectorized Data\n",
    "- Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawSurnames(object):\n",
    "    def __init__(self, data_path, delimiter=\",\"):\n",
    "        self.data = pd.read_csv(data_path, delimiter=delimiter)\n",
    "\n",
    "    def get_data(self, filter_to_nationality=None):\n",
    "        if filter_to_nationality is not None:\n",
    "            return self.data[self.data.nationality.isin(filter_to_nationality)]\n",
    "        return self.data\n",
    "\n",
    "# vectorizer\n",
    "\n",
    "class SurnamesVectorizer(object):\n",
    "    def __init__(self, surname_vocab, nationality_vocab, max_seq_length):\n",
    "        self.surname_vocab = surname_vocab\n",
    "        self.nationality_vocab = nationality_vocab\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    def save(self, filename):\n",
    "        vec_dict = {\"surname_vocab\": self.surname_vocab.get_serializable_contents(),\n",
    "                    \"nationality_vocab\": self.nationality_vocab.get_serializable_contents(),\n",
    "                    'max_seq_length': self.max_seq_length}\n",
    "\n",
    "        with open(filename, \"w\") as fp:\n",
    "            json.dump(vec_dict, fp)\n",
    "        \n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        with open(filename, \"r\") as fp:\n",
    "            vec_dict = json.load(fp)\n",
    "\n",
    "        vec_dict[\"surname_vocab\"] = Vocabulary.deserialize_from_contents(vec_dict[\"surname_vocab\"])\n",
    "        vec_dict[\"nationality_vocab\"] = Vocabulary.deserialize_from_contents(vec_dict[\"nationality_vocab\"])\n",
    "        return cls(**vec_dict)\n",
    "\n",
    "    @classmethod\n",
    "    def fit(cls, surname_df):\n",
    "        surname_vocab = Vocabulary(use_unks=False,\n",
    "                                   use_mask=True,\n",
    "                                   use_start_end=True,\n",
    "                                   start_token=START_TOKEN,\n",
    "                                   end_token=END_TOKEN)\n",
    "\n",
    "        nationality_vocab = Vocabulary(use_unks=False, use_start_end=False, use_mask=False)\n",
    "\n",
    "        max_seq_length = 0\n",
    "        for index, row in surname_df.iterrows():\n",
    "            surname_vocab.add_many(row.surname)\n",
    "            nationality_vocab.add(row.nationality)\n",
    "\n",
    "            if len(row.surname) > max_seq_length:\n",
    "                max_seq_length = len(row.surname)\n",
    "        max_seq_length = max_seq_length + 2\n",
    "\n",
    "        return cls(surname_vocab, nationality_vocab, max_seq_length)\n",
    "\n",
    "    @classmethod\n",
    "    def fit_transform(cls, surname_df, split='train'):\n",
    "        vectorizer = cls.fit(surname_df)\n",
    "        return vectorizer, vectorizer.transform(surname_df, split)\n",
    "    \n",
    "    def transform(self, surname_df, split='train'):\n",
    "\n",
    "        df = surname_df[surname_df.split==split].reset_index()\n",
    "        n_data = len(df)\n",
    "        \n",
    "        x_surnames = np.zeros((n_data, self.max_seq_length), dtype=np.int64)\n",
    "        y_surnames = np.ones((n_data, self.max_seq_length), dtype=np.int64) * IGNORE_INDEX_VALUE\n",
    "        x_nationalities = np.zeros(n_data, dtype=np.int64)\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            vectorized_surname = list(self.surname_vocab.map(row.surname, \n",
    "                                                             include_start_end=True))\n",
    "            x_part = vectorized_surname[:-1]\n",
    "            y_part = vectorized_surname[1:]\n",
    "            x_surnames[index, :len(x_part)] = x_part\n",
    "            y_surnames[index, :len(y_part)] = y_part\n",
    "            x_nationalities[index] = self.nationality_vocab[row.nationality]\n",
    "\n",
    "        return VectorizedSurnames(x_surnames, x_nationalities, y_surnames)\n",
    "\n",
    "# vec data\n",
    "\n",
    "class VectorizedSurnames(Dataset):\n",
    "    def __init__(self, x_surnames, x_nationalities, y_surnames):\n",
    "        self.x_surnames = x_surnames\n",
    "        self.x_nationalities = x_nationalities\n",
    "        self.y_surnames = y_surnames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_surnames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {'x_surnames': self.x_surnames[index],\n",
    "                'x_nationalities': self.x_nationalities[index],\n",
    "                'y_surnames': self.y_surnames[index],\n",
    "                'x_lengths': len(self.x_surnames[index].nonzero()[0])}\n",
    "\n",
    "# data generator\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, use_cuda=False):\n",
    "        \n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = Variable(tensor)\n",
    "            if use_cuda:\n",
    "                out_data_dict[name] = out_data_dict[name].cuda()\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class definitions for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_parameter(*size):\n",
    "    out = Parameter(torch.FloatTensor(*size))\n",
    "    torch.nn.init.xavier_normal(out)\n",
    "    return out\n",
    "\n",
    "def column_gather(y_out, x_lengths):\n",
    "    '''Get a specific vector from each batch datapoint in `y_out`.\n",
    "\n",
    "    More precisely, iterate over batch row indices, get the vector that's at\n",
    "    the position indicated by the corresponding value in `x_lengths` at the row\n",
    "    index. \n",
    "\n",
    "    Args:\n",
    "        y_out (torch.FloatTensor, torch.cuda.FloatTensor)\n",
    "            shape: (batch, sequence, feature)\n",
    "        x_lengths (torch.LongTensor, torch.cuda.LongTensor)\n",
    "            shape: (batch,)\n",
    "\n",
    "    Returns:\n",
    "        y_out (torch.FloatTensor, torch.cuda.FloatTensor)\n",
    "            shape: (batch, feature)\n",
    "    '''\n",
    "    x_lengths = x_lengths.long().data.cpu().numpy() - 1\n",
    "    # alternatively:\n",
    "    # out = []\n",
    "    # for batch_index, column_index in enumerate(x_lengths):\n",
    "    #     out.append(y_out[batch_index, column_index])\n",
    "    # return torch.stack(out)\n",
    "    return torch.stack([y_out[batch_index, column_index]\n",
    "                        for batch_index, column_index in enumerate(x_lengths)])\n",
    "\n",
    "class ExplicitRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_first=False):\n",
    "        super(ExplicitRNN, self).__init__()\n",
    "        self.W_in2hid = new_parameter(input_size, hidden_size)\n",
    "        self.W_hid2hid = new_parameter(hidden_size, hidden_size)\n",
    "            \n",
    "        self.b_hid = new_parameter(1, hidden_size)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "    \n",
    "    def _compute_next_hidden(self, x, h):\n",
    "        return F.tanh(x.matmul(self.W_in2hid) + \n",
    "                      h.matmul(self.W_hid2hid) + \n",
    "                      self.b_hid)\n",
    "\n",
    "    def forward(self, x_in, hid_t=None):\n",
    "        if self.batch_first:\n",
    "            batch_size, seq_size, feat_size = x_in.size()\n",
    "            x_in = x_in.permute(1, 0, 2)\n",
    "        else:\n",
    "            seq_size, batch_size, feat_size = x_in.size()\n",
    "\n",
    "        hiddens = []\n",
    "        if hid_t is None:\n",
    "            hid_t = Variable(torch.zeros((batch_size, self.hidden_size)))\n",
    "        \n",
    "        if x_in.is_cuda:\n",
    "            hid_t = hid_t.cuda()\n",
    "            \n",
    "        for t in range(seq_size):\n",
    "            x_t = x_in[t]\n",
    "            hid_t = self._compute_next_hidden(x_t, hid_t)\n",
    "            \n",
    "            hiddens.append(hid_t)\n",
    "        hiddens = torch.stack(hiddens)\n",
    "\n",
    "        if self.batch_first:\n",
    "            hiddens = hiddens.permute(1, 0, 2)\n",
    "\n",
    "        return hiddens\n",
    "    \n",
    "class CharNN(nn.Module):\n",
    "    def __init__(self, embedding_size, in_vocab_size, out_vocab_size, hidden_size, num_conditioning_states,\n",
    "                 batch_first=False):\n",
    "        super(CharNN, self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(embedding_dim=embedding_size, \n",
    "                                num_embeddings=in_vocab_size, \n",
    "                                padding_idx=0)\n",
    "        self.conditional_emb = nn.Embedding(embedding_dim=hidden_size, \n",
    "                                            num_embeddings=num_conditioning_states)\n",
    "        self.fc = nn.Linear(in_features=hidden_size, out_features=out_vocab_size)\n",
    "        self.rnn = ExplicitRNN(input_size=embedding_size, hidden_size=hidden_size, \n",
    "                               batch_first=batch_first)\n",
    "    \n",
    "    def forward(self, x_in, state_in, x_lengths=None, apply_softmax=False):\n",
    "        x_in = self.emb(x_in)\n",
    "        state_in = self.conditional_emb(state_in)\n",
    "        y_out = self.rnn(x_in, state_in)\n",
    "\n",
    "        dim0, dim1, dim2 = y_out.size()\n",
    "        y_out = y_out.contiguous().view(-1, dim2)\n",
    "\n",
    "        y_out = self.fc(y_out)\n",
    "\n",
    "        # optionally apply the softmax\n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "\n",
    "        y_out = y_out.view(dim0, dim1, -1)\n",
    "        \n",
    "        return y_out\n",
    "    \n",
    "def normalize_sizes(net_output, y_true):\n",
    "    net_output = net_output.cpu()\n",
    "    y_true = y_true.cpu()\n",
    "    if len(net_output.size()) == 3:\n",
    "        net_output.contiguous()\n",
    "        net_output = net_output.view(-1, net_output.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true.contiguous()\n",
    "        y_true = y_true.view(-1)\n",
    "    return net_output, y_true\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "\n",
    "    _, y_pred_indices = y_pred.cpu().max(dim=1)\n",
    "    \n",
    "    correct_indices = torch.eq(y_pred_indices, y_true).float()\n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "    \n",
    "    n_correct = (correct_indices * valid_indices).sum().data[0]\n",
    "    n_valid = valid_indices.sum().data[0]\n",
    "\n",
    "    return n_correct / n_valid * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not enabled\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    surname_csv=\"data/surnames.csv\",\n",
    "    batch_size = 128,\n",
    "    cuda=False,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=100,\n",
    "    load_zoo_model=True,\n",
    "    zoo={\n",
    "        'filename': 'modelzoo/charnn_emb16_hid64_surnames_conditionally_predict.state',\n",
    "        'vocab': 'modelzoo/surnames_classify.vocab',\n",
    "        'comments': 'pre-trained surname conditioned sequence prediction (& conditioned generation)',\n",
    "        'parameters': {\n",
    "            'embedding_size': 16,\n",
    "            'hidden_size': 64\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "args.cuda = args.cuda and torch.cuda.is_available()\n",
    "\n",
    "if args.cuda:\n",
    "    print(\"CUDA is enabled\")\n",
    "else:\n",
    "    print(\"CUDA is not enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: set this to false to learn from scratch!\n",
    "# args.load_zoo_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectorizer!\n",
      "Loading state dict!\n"
     ]
    }
   ],
   "source": [
    "raw_data = RawSurnames(args.surname_csv).get_data()\n",
    "\n",
    "if os.path.exists(args.zoo['vocab']):\n",
    "    vectorizer = SurnamesVectorizer.load(args.zoo['vocab'])\n",
    "    print(\"Loading vectorizer!\")\n",
    "else:\n",
    "    vectorizer = SurnamesVectorizer.fit(raw_data)\n",
    "    print(\"Creating a new vectorizer.\")\n",
    "    \n",
    "    \n",
    "train_dataset = vectorizer.transform(raw_data, split='train')\n",
    "test_dataset = vectorizer.transform(raw_data, split='test')\n",
    "\n",
    "\n",
    "zoo_params = args.zoo['parameters']\n",
    "\n",
    "net = CharNN(embedding_size=zoo_params['embedding_size'], \n",
    "             hidden_size=zoo_params['hidden_size'],\n",
    "             in_vocab_size=len(vectorizer.surname_vocab), \n",
    "             out_vocab_size=len(vectorizer.surname_vocab), \n",
    "             num_conditioning_states=len(vectorizer.nationality_vocab),\n",
    "             batch_first=True)\n",
    "\n",
    "if args.load_zoo_model and os.path.exists(args.zoo['filename']):\n",
    "    print(\"Loading state dict!\")\n",
    "    net.load_state_dict(torch.load(args.zoo['filename'], \n",
    "                                   map_location=lambda storage, loc: storage))\n",
    "else:\n",
    "    print(\"Using newly initiated network!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA mode not enabled\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e30edbc618f4c9a841ac1ed0a0ef78e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epochs'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb66e4aa373d4007bb3300f9a18414b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training', max=125), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc451d582f34cb4ab27f3d1a45c8f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test', max=31), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    }
   ],
   "source": [
    "if args.cuda:\n",
    "    print(\"CUDA mode enabled\")\n",
    "    net = net.cuda()\n",
    "else:\n",
    "    print(\"CUDA mode not enabled\")\n",
    "    net = net.cpu()\n",
    "    \n",
    "# optimizer \n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=args.learning_rate)\n",
    "\n",
    "# loss function\n",
    "\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)\n",
    "\n",
    "# progress bars\n",
    "\n",
    "epoch_bar = tqdm_notebook(desc='epochs', total=args.num_epochs, position=1)\n",
    "\n",
    "num_train_batches = len(train_dataset) // args.batch_size\n",
    "train_bar = tqdm_notebook(desc='training', total=num_train_batches, position=2)\n",
    "\n",
    "num_test_batches = len(test_dataset) // args.batch_size\n",
    "test_bar = tqdm_notebook(desc='test', total=num_test_batches, position=3)\n",
    "\n",
    "# history\n",
    "\n",
    "train_loss_history = []\n",
    "train_accuracy_history = []\n",
    "\n",
    "test_loss_history = []\n",
    "test_accuracy_history = []\n",
    "\n",
    "\n",
    "try:\n",
    "    for _ in range(args.num_epochs):\n",
    "        batch_generator = generate_batches(train_dataset, batch_size=args.batch_size)\n",
    "        \n",
    "        per_epoch_loss = []\n",
    "        per_epoch_accuracy = []\n",
    "        \n",
    "        net.train()\n",
    "            \n",
    "        for batch_dict in batch_generator:\n",
    "            # step 1\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # step 2\n",
    "            y_pred = net(batch_dict['x_surnames'], \n",
    "                         batch_dict['x_nationalities'],\n",
    "                         batch_dict['x_lengths'])\n",
    "            y_target = batch_dict['y_surnames'] \n",
    "            \n",
    "            # step 3\n",
    "            loss = sequence_loss(y_pred, y_target, IGNORE_INDEX_VALUE)\n",
    "\n",
    "            # step 4\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # bonus steps: bookkeeping\n",
    "            per_epoch_loss.append(loss.cpu().data[0])        \n",
    "        \n",
    "            accuracy = compute_accuracy(y_pred, batch_dict['y_surnames'], IGNORE_INDEX_VALUE)\n",
    "            per_epoch_accuracy.append(accuracy)\n",
    "\n",
    "            train_bar.update()\n",
    "            \n",
    "            train_bar.set_postfix(loss=per_epoch_loss[-1], \n",
    "                                  accuracy=per_epoch_accuracy[-1])\n",
    "            \n",
    "        train_loss_history.append(np.mean(per_epoch_loss))\n",
    "        train_accuracy_history.append(np.mean(per_epoch_accuracy))\n",
    "        \n",
    "        # loop over test dataset\n",
    "        \n",
    "        batch_generator = generate_batches(test_dataset, batch_size=args.batch_size)\n",
    "        \n",
    "        per_epoch_loss = []\n",
    "        per_epoch_accuracy = []\n",
    "            \n",
    "        # set it to eval mode; this turns stochastic functions off\n",
    "        net.eval()\n",
    "            \n",
    "        for batch_dict in batch_generator:\n",
    "            # step 1: compute output\n",
    "            y_pred = net(batch_dict['x_surnames'], \n",
    "                         batch_dict['x_nationalities'],\n",
    "                         batch_dict['x_lengths'])\n",
    "            y_target = batch_dict['y_surnames'] \n",
    "            \n",
    "            # step 2: compute metrics\n",
    "            loss = sequence_loss(y_pred, y_target, IGNORE_INDEX_VALUE)\n",
    "            per_epoch_loss.append(loss.cpu().data[0])\n",
    "          \n",
    "            accuracy = compute_accuracy(y_pred, batch_dict['y_surnames'], IGNORE_INDEX_VALUE)\n",
    "            per_epoch_accuracy.append(accuracy)\n",
    "\n",
    "            test_bar.update()\n",
    "            \n",
    "            test_bar.set_postfix(loss=per_epoch_loss[-1], \n",
    "                                 accuracy=per_epoch_accuracy[-1])\n",
    "            \n",
    "        test_loss_history.append(np.mean(per_epoch_loss))\n",
    "        test_accuracy_history.append(np.mean(per_epoch_accuracy))\n",
    "        \n",
    "        # update bars\n",
    "        \n",
    "        epoch_bar.set_postfix(train_loss=train_loss_history[-1], \n",
    "                              train_accuracy=train_accuracy_history[-1],\n",
    "                              test_loss=test_loss_history[-1],\n",
    "                              test_accuracy=test_accuracy_history[-1])\n",
    "        epoch_bar.update()\n",
    "        test_bar.n = 0\n",
    "        train_bar.n = 0\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "In the plain prediction notebook, the exercise was to create the sampling routine. Below is code already written to sample, so you can see how to handle the conditional nature.  \n",
    "\n",
    "As an exercise, you should consider what it would mean to interpolate between two conditioning vectors!  \n",
    "\n",
    "For instance, you could take the Irish and Chinese embeddings and average them (which is the same thing as multiplying each vector by 0.5 and adding them together!). \n",
    "\n",
    "Or you could weight them 0.3 and 0.7 and then add them together.  This is referred to as interpolation.  The weights should add up to 1 to be a valid interpolation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(emb, rnn, fc, h_t=None, idx_t=None, n=20, temp=1):\n",
    "    hiddens = [h_t]\n",
    "    indices = [idx_t]\n",
    "    out_dists = []\n",
    "    \n",
    "    for t in range(n):\n",
    "        x_t = emb(idx_t)\n",
    "        h_t = rnn._compute_next_hidden(x_t, h_t)\n",
    "        \n",
    "        y_t = fc(h_t)\n",
    "        y_t = F.softmax( y_t / temp, dim=1)\n",
    "        idx_t = torch.multinomial(y_t, 1)[:, 0]\n",
    "        \n",
    "        \n",
    "        hiddens.append(h_t)\n",
    "        indices.append(idx_t)\n",
    "        out_dists.append(y_t)\n",
    "     \n",
    "    indices = torch.stack(indices).squeeze().permute(1, 0)\n",
    "    return indices\n",
    "\n",
    "def long_variable(indices):\n",
    "    out = Variable(torch.LongTensor(indices))\n",
    "    return out\n",
    "\n",
    "def make_initial_hidden(batch_size, hidden_size):\n",
    "    out = Variable(torch.ones(batch_size, hidden_size))\n",
    "    return out\n",
    "\n",
    "def make_initial_x(batch_size, vectorizer):\n",
    "    out = Variable(torch.ones(batch_size) * vectorizer.surname_vocab.start_index).long()\n",
    "    return out\n",
    "\n",
    "def decode_one(vectorizer, seq):\n",
    "    out = []\n",
    "    for i in seq:\n",
    "        if vectorizer.surname_vocab.start_index == i:\n",
    "            continue\n",
    "        if vectorizer.surname_vocab.end_index == i:\n",
    "            return ''.join(out)\n",
    "        out.append(vectorizer.surname_vocab.lookup(i))\n",
    "    return ''.join(out)\n",
    "            \n",
    "def decode_matrix(vectorizer, mat):\n",
    "    mat = mat.cpu().data.numpy()\n",
    "    return [decode_one(vectorizer, mat[i]) for i in range(len(mat))]\n",
    "\n",
    "def n_random_nationalities(n):\n",
    "    keys = np.random.choice(vectorizer.nationality_vocab.keys(), size=n, replace=True)\n",
    "    indices = long_variable([vectorizer.nationality_vocab[key] for key in keys])\n",
    "    return keys, indices\n",
    "\n",
    "def sample_n(n=10, temp=0.8):\n",
    "    init_names, init_vector = n_random_nationalities(n)\n",
    "    init_vector = net.conditional_emb(init_vector)\n",
    "    samples = decode_matrix(vectorizer, \n",
    "                            sample(net.emb, net.rnn, net.fc, \n",
    "                                   init_vector, \n",
    "                                   make_initial_x(n, vectorizer),\n",
    "                                   temp=temp))\n",
    "    return list(zip(init_names, samples))\n",
    "\n",
    "def sample_n_for_nationality(nationality, n=10, temp=0.8):\n",
    "    assert nationality in vectorizer.nationality_vocab.keys(), 'not a nationality we trained on'\n",
    "    keys = [nationality] * n\n",
    "    init_vector = long_variable([vectorizer.nationality_vocab[key] for key in keys])\n",
    "    init_vector = net.conditional_emb(init_vector)\n",
    "    samples = decode_matrix(vectorizer, \n",
    "                        sample(net.emb, net.rnn, net.fc, \n",
    "                               init_vector, \n",
    "                               make_initial_x(n, vectorizer),\n",
    "                               temp=temp))\n",
    "    return list(zip(keys, samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('japanese', 'Tikadi'),\n",
       " ('japanese', 'Yasaraka'),\n",
       " ('japanese', 'Kanesa'),\n",
       " ('japanese', 'Obanai'),\n",
       " ('japanese', 'Kokiri'),\n",
       " ('japanese', 'Yohriy'),\n",
       " ('japanese', 'Kanuda'),\n",
       " ('japanese', 'Tase'),\n",
       " ('japanese', 'Kura'),\n",
       " ('japanese', 'Furakai')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_n_for_nationality('japanese', n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('portuguese', 'Corto'),\n",
       " ('korean', 'Cheng'),\n",
       " ('portuguese', 'Manosso'),\n",
       " ('dutch', 'Rean'),\n",
       " ('english', 'Ory'),\n",
       " ('dutch', 'Sweilts'),\n",
       " ('irish', 'Bnight'),\n",
       " ('vietnamese', 'Hai'),\n",
       " ('chinese', 'Shan'),\n",
       " ('french', 'Pariem')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_n()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vietnamese', 'Then'),\n",
       " ('japanese', 'Akiya'),\n",
       " ('irish', \"D'xad\"),\n",
       " ('dutch', 'Horns'),\n",
       " ('italian', 'Carbona'),\n",
       " ('portuguese', 'Sarta'),\n",
       " ('french', 'Michanda'),\n",
       " ('polish', 'Sakilo'),\n",
       " ('spanish', 'Vorcos'),\n",
       " ('greek', 'Gocoana'),\n",
       " ('french', 'Sollo'),\n",
       " ('english', 'Thurlman'),\n",
       " ('czech', 'Bagenkovi'),\n",
       " ('italian', 'Aglinosarra'),\n",
       " ('english', 'Elwing'),\n",
       " ('dutch', 'Kohmin'),\n",
       " ('german', 'Rarkon'),\n",
       " ('chinese', 'Huean'),\n",
       " ('german', 'Silpin'),\n",
       " ('french', 'Mals'),\n",
       " ('russian', 'Livonov'),\n",
       " ('french', 'Gules'),\n",
       " ('english', 'Rulbelt'),\n",
       " ('czech', 'Kodler'),\n",
       " ('czech', 'Clestertov'),\n",
       " ('dutch', 'Leanan'),\n",
       " ('irish', \"O'Coux\"),\n",
       " ('czech', 'Kradberba'),\n",
       " ('irish', 'Malran'),\n",
       " ('german', 'Kamer')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_n(30, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise!\n",
    "\n",
    "Can you figure out how to take the embedding for TWO nationalities, average them, and use that to generate a new surname? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
